class LogisticRegressor:
    import numpy as np

    def __init__(self, epochs=10, batch=10, learningrate=.01):
        from sklearn.utils import shuffle
        self.np = np
        self.shuffle = shuffle
        self.epoch = epochs
        self.batch = batch
        self.lr = learningrate

    def addBias(self, T):
        bias = np.concatenate((self.np.ones(len(T)).reshape(-1,1),T),axis=1)
        return bias

    def batchgen(self, X, Y, batchsize):
        for i in self.np.arange(0, X.shape[0], batchsize):
            yield (X[i:i + batchsize], Y[i:i + batchsize])

    def fit(self, features, labels, progress_reporting=5):
        count = 0
        lr = self.lr/len(labels)
        self.stdy = np.std(labels)
        self.meany = np.mean(labels)
        X = self.addBias(features)
        Y = labels.reshape(-1,1)
        self.B = self.np.random.uniform(0,1,(features.shape[1]+1,1))
        for epoch in range(self.epoch):
            xs, ys = self.shuffle(features,Y) 
            for x, y in self.batchgen(xs, ys, self.batch):
                yhat = self.prob(x)
                deltaB = self.np.matmul(self.addBias(x).T,((yhat-y))*yhat*(1-yhat))
                self.B = self.B - lr * deltaB
                count = count + 1
                if count%progress_reporting == 0 or count == len(labels):
                    print("Count: ", count)
                    print("Score: ", self.score(features,labels))
                    print("* * * * * * * * * *")

    def sigmoid(self, x):
        import math
        sigmoid = 1 / (1 + (math.e ** -x))
        return sigmoid
    
    def predict(self, test):
        yhat = self.sigmoid(self.np.matmul(self.addBias(test), self.B))
        prediction = self.np.round(yhat, 0)
        return prediction
    
    def prob(self, test):
        yhat = self.sigmoid(self.np.matmul(self.addBias(test), self.B))
        return yhat
    
    def score(self, x, y):
        yhat = self.predict(x)
        error = abs(yhat - y.reshape(-1,1))
        score = 1 - np.mean(error)
        return score.round(4)
       
from joblib import Memory
from sklearn.datasets import load_svmlight_file
from scipy.sparse import csc_matrix
from scipy.special import expit
from scipy import sparse
from sklearn.preprocessing import normalize
import numpy as np
import pandas as pd
from copy import copy
from sklearn import svm
import time
mem = Memory("./mycache")

def get_data(datafile):
    data = load_svmlight_file(datafile)
    return data[0], data[1]

X, Z = get_data('a1a.t')

x_train = pd.DataFrame(X.todense())

features = x_train.shape[1]
x_train[features] = 0

m = min(Z)
M = max(Z)
y_train = (Z-m)/(M-m)

y_train.shape=(len(y_train),1)

T, u = get_data('a1a.t')

x_test = pd.DataFrame(T.todense())
x_test[features] = 0
x_test = x_test.iloc[:,0:(features+1)]

m = min(u)
M = max(u)
y_test = (u-m)/(M-m)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

lr = LogisticRegressor(epochs=20, learningrate=.7, batch=20)

lr.fit(x_train, y_train, progress_reporting=100)

lr.predict(x_test)
lr.prob(x_test)
lr.prob(x_test)
lr.sigmoid(x_test)
